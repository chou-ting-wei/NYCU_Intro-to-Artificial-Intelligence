{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVIlLeHyupij",
        "outputId": "661b9979-2ffe-49f9-ec70-8c96a6d2fb12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ar95QtOLupik",
        "outputId": "ff43b4f7-9d3c-4e9c-b9bb-c5caada0e4bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/gdrive/MyDrive/HW2.zip\n",
            "   creating: HW2/\n",
            "  inflating: HW2/ngram.py            \n",
            "  inflating: __MACOSX/HW2/._ngram.py  \n",
            "  inflating: HW2/.DS_Store           \n",
            "  inflating: __MACOSX/HW2/._.DS_Store  \n",
            "  inflating: HW2/preprocess.py       \n",
            "  inflating: __MACOSX/HW2/._preprocess.py  \n",
            "  inflating: HW2/run_local.ipynb     \n",
            "   creating: HW2/__pycache__/\n",
            "  inflating: HW2/run_colab.ipynb     \n",
            "  inflating: HW2/bert.py             \n",
            "  inflating: __MACOSX/HW2/._bert.py  \n",
            "  inflating: HW2/main.py             \n",
            "  inflating: __MACOSX/HW2/._main.py  \n",
            "   creating: HW2/data/\n",
            "  inflating: __MACOSX/HW2/._data     \n",
            "  inflating: HW2/rnn.py              \n",
            "  inflating: __MACOSX/HW2/._rnn.py   \n",
            "  inflating: HW2/__pycache__/preprocess.cpython-310.pyc  \n",
            "  inflating: HW2/__pycache__/ngram.cpython-310.pyc  \n",
            "  inflating: HW2/__pycache__/rnn.cpython-310.pyc  \n",
            "  inflating: HW2/__pycache__/bert.cpython-310.pyc  \n",
            "  inflating: HW2/data/IMDB_test.csv  \n",
            "  inflating: __MACOSX/HW2/data/._IMDB_test.csv  \n",
            "  inflating: HW2/data/IMDB_train.csv  \n",
            "  inflating: __MACOSX/HW2/data/._IMDB_train.csv  \n",
            "/content/HW2\n"
          ]
        }
      ],
      "source": [
        "!unzip /content/gdrive/MyDrive/HW2.zip\n",
        "%cd /content/HW2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbPcdIEBupik",
        "outputId": "b4eb41eb-bfba-4238-b638-ee0d61f237ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLjo0P-smD8o",
        "outputId": "dca764e8-85cc-4800-8aea-27cdb8f9cac1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import nltk\n",
            "nltk.download('stopwords')\n",
            "nltk.download('punkt')\n",
            "\n",
            "from nltk.corpus import stopwords\n",
            "from nltk.tokenize.toktok import ToktokTokenizer\n",
            "\n",
            "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
            "\n",
            "stop_word_list = stopwords.words('english')\n",
            "\n",
            "def remove_stopwords(text: str) -> str:\n",
            "    '''\n",
            "    E.g.,\n",
            "        text: 'Here is a dog.'\n",
            "        preprocessed_text: 'Here dog.'\n",
            "    '''\n",
            "    tokenizer = ToktokTokenizer()\n",
            "    tokens = tokenizer.tokenize(text)\n",
            "    tokens = [token.strip() for token in tokens]\n",
            "    filtered_tokens = [token for token in tokens if token.lower() not in stop_word_list]\n",
            "    preprocessed_text = ' '.join(filtered_tokens)\n",
            "\n",
            "    return preprocessed_text\n",
            "\n",
            "\n",
            "def preprocessing_function(text: str) -> str:\n",
            "    preprocessed_text = remove_stopwords(text)\n",
            "    \n",
            "    # TO-DO 0: Other preprocessing function attemption\n",
            "    # Begin your code \n",
            "    # preprocessed_text = preprocessed_text.lower()\n",
            "    # preprocessed_text = preprocessed_text.replace('<br / >', ' ')\n",
            "    # preprocessed_text = ''.join([char for char in preprocessed_text if char.isalpha() or char.isspace()])\n",
            "    \n",
            "    # words = nltk.word_tokenize(preprocessed_text)\n",
            "    \n",
            "    # PorterStemmer() SnowballStemmer() LancasterStemmer()\n",
            "    # wordsPorter = [PorterStemmer().stem(word) for word in words]\n",
            "    # wordsSnowball = [SnowballStemmer(language='english').stem(word) for word in words]\n",
            "    # wordsLancaster = [LancasterStemmer().stem(word) for word in words]\n",
            "    \n",
            "    # print(f'Original:\\n{text}\\n'\n",
            "    #       f'Remove stopwords/symbols:\\n{\" \".join(words)}\\n'\n",
            "    #       f'PorterStemmer:\\n{\" \".join(wordsPorter)}\\n'\n",
            "    #       f'SnowballStemmer:\\n{\" \".join(wordsSnowball)}\\n'\n",
            "    #       f'LancasterStemmer:\\n{\" \".join(wordsLancaster)}')\n",
            "    \n",
            "    # stemmed_words = [SnowballStemmer(language='english').stem(word) for word in words]\n",
            "    # preprocessed_text = ' '.join(stemmed_words)\n",
            "    # End your code\n",
            "\n",
            "    return preprocessed_text"
          ]
        }
      ],
      "source": [
        "!cat preprocess.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWqGrSSsgl6o",
        "outputId": "1335bc52-dacb-4abb-f2c8-da1ce98b3aa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Epoch 1/10: 100% 5000/5000 [02:13<00:00, 37.51batch/s, loss=0.661]\n",
            "Evaluating: 100% 10000/10000 [00:14<00:00, 685.25batch/s]\n",
            "Epoch: 1, Loss: 0.6929, Precision: 0.5761, Recall: 0.5690, F1: 0.5587\n",
            "Epoch 2/10: 100% 5000/5000 [02:11<00:00, 38.01batch/s, loss=0.661]\n",
            "Evaluating: 100% 10000/10000 [00:13<00:00, 757.14batch/s]\n",
            "Epoch: 2, Loss: 0.6387, Precision: 0.7601, Recall: 0.7581, F1: 0.7576\n",
            "Epoch 3/10: 100% 5000/5000 [02:11<00:00, 37.98batch/s, loss=0.689]\n",
            "Evaluating: 100% 10000/10000 [00:13<00:00, 753.34batch/s]\n",
            "Epoch: 3, Loss: 0.5427, Precision: 0.7611, Recall: 0.7563, F1: 0.7552\n",
            "Epoch 4/10: 100% 5000/5000 [02:12<00:00, 37.87batch/s, loss=0.439]\n",
            "Evaluating: 100% 10000/10000 [00:13<00:00, 756.05batch/s]\n",
            "Epoch: 4, Loss: 0.5129, Precision: 0.8009, Recall: 0.7997, F1: 0.7995\n",
            "Epoch 5/10: 100% 5000/5000 [02:11<00:00, 37.89batch/s, loss=0.441]\n",
            "Evaluating: 100% 10000/10000 [00:13<00:00, 755.35batch/s]\n",
            "Epoch: 5, Loss: 0.4890, Precision: 0.8158, Recall: 0.8158, F1: 0.8158\n",
            "Epoch 6/10: 100% 5000/5000 [02:12<00:00, 37.67batch/s, loss=0.434]\n",
            "Evaluating: 100% 10000/10000 [00:13<00:00, 738.52batch/s]\n",
            "Epoch: 6, Loss: 0.4641, Precision: 0.8101, Recall: 0.8100, F1: 0.8100\n",
            "Epoch 7/10: 100% 5000/5000 [02:12<00:00, 37.84batch/s, loss=0.444]\n",
            "Evaluating: 100% 10000/10000 [00:13<00:00, 751.64batch/s]\n",
            "Epoch: 7, Loss: 0.4524, Precision: 0.8139, Recall: 0.7926, F1: 0.7890\n",
            "Epoch 8/10: 100% 5000/5000 [02:13<00:00, 37.51batch/s, loss=0.314]\n",
            "Evaluating: 100% 10000/10000 [00:14<00:00, 708.97batch/s]\n",
            "Epoch: 8, Loss: 0.4341, Precision: 0.8312, Recall: 0.8281, F1: 0.8277\n",
            "Epoch 9/10: 100% 5000/5000 [02:12<00:00, 37.71batch/s, loss=0.554]\n",
            "Evaluating: 100% 10000/10000 [00:13<00:00, 748.03batch/s]\n",
            "Epoch: 9, Loss: 0.4256, Precision: 0.8242, Recall: 0.8163, F1: 0.8152\n",
            "Epoch 10/10: 100% 5000/5000 [02:12<00:00, 37.73batch/s, loss=0.313]\n",
            "Evaluating: 100% 10000/10000 [00:13<00:00, 749.65batch/s]\n",
            "Epoch: 10, Loss: 0.4190, Precision: 0.8427, Recall: 0.8397, F1: 0.8393\n"
          ]
        }
      ],
      "source": [
        "!python main.py --model_type RNN --preprocess 0 --part 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmaXcy_jgl6p",
        "outputId": "ed9d2c62-1a2f-47c2-bf8f-d552b0d1d978"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Preprocessing train data: 100% 40000/40000 [00:37<00:00, 1068.48it/s]\n",
            "Preprocessing test data: 100% 10000/10000 [00:09<00:00, 1050.59it/s]\n",
            "Epoch 1/10: 100% 5000/5000 [02:09<00:00, 38.63batch/s, loss=0.657]\n",
            "Evaluating: 100% 10000/10000 [00:11<00:00, 851.08batch/s]\n",
            "Epoch: 1, Loss: 0.6896, Precision: 0.7035, Recall: 0.7035, F1: 0.7035\n",
            "Epoch 2/10: 100% 5000/5000 [02:08<00:00, 38.87batch/s, loss=0.626]\n",
            "Evaluating: 100% 10000/10000 [00:11<00:00, 900.82batch/s]\n",
            "Epoch: 2, Loss: 0.5438, Precision: 0.8116, Recall: 0.8098, F1: 0.8095\n",
            "Epoch 3/10: 100% 5000/5000 [02:08<00:00, 38.90batch/s, loss=0.535]\n",
            "Evaluating: 100% 10000/10000 [00:11<00:00, 889.44batch/s]\n",
            "Epoch: 3, Loss: 0.4675, Precision: 0.8301, Recall: 0.8291, F1: 0.8290\n",
            "Epoch 4/10: 100% 5000/5000 [02:08<00:00, 38.94batch/s, loss=0.314]\n",
            "Evaluating: 100% 10000/10000 [00:10<00:00, 934.80batch/s]\n",
            "Epoch: 4, Loss: 0.4392, Precision: 0.8352, Recall: 0.8352, F1: 0.8352\n",
            "Epoch 5/10: 100% 5000/5000 [02:08<00:00, 38.94batch/s, loss=0.314]\n",
            "Evaluating: 100% 10000/10000 [00:11<00:00, 880.51batch/s]\n",
            "Epoch: 5, Loss: 0.4219, Precision: 0.8431, Recall: 0.8417, F1: 0.8415\n",
            "Epoch 6/10: 100% 5000/5000 [02:09<00:00, 38.73batch/s, loss=0.437]\n",
            "Evaluating: 100% 10000/10000 [00:11<00:00, 890.18batch/s]\n",
            "Epoch: 6, Loss: 0.4067, Precision: 0.8499, Recall: 0.8496, F1: 0.8496\n",
            "Epoch 7/10: 100% 5000/5000 [02:08<00:00, 38.88batch/s, loss=0.438]\n",
            "Evaluating: 100% 10000/10000 [00:11<00:00, 902.70batch/s]\n",
            "Epoch: 7, Loss: 0.3933, Precision: 0.8523, Recall: 0.8511, F1: 0.8510\n",
            "Epoch 8/10: 100% 5000/5000 [02:09<00:00, 38.67batch/s, loss=0.313]\n",
            "Evaluating: 100% 10000/10000 [00:11<00:00, 899.02batch/s]\n",
            "Epoch: 8, Loss: 0.3841, Precision: 0.8565, Recall: 0.8561, F1: 0.8561\n",
            "Epoch 9/10: 100% 5000/5000 [02:09<00:00, 38.71batch/s, loss=0.563]\n",
            "Evaluating: 100% 10000/10000 [00:10<00:00, 912.11batch/s]\n",
            "Epoch: 9, Loss: 0.3808, Precision: 0.8562, Recall: 0.8517, F1: 0.8512\n",
            "Epoch 10/10: 100% 5000/5000 [02:08<00:00, 38.87batch/s, loss=0.313]\n",
            "Evaluating: 100% 10000/10000 [00:11<00:00, 888.23batch/s]\n",
            "Epoch: 10, Loss: 0.3760, Precision: 0.8552, Recall: 0.8488, F1: 0.8481\n"
          ]
        }
      ],
      "source": [
        "!python main.py --model_type RNN --preprocess 1 --part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT - Version 1"
      ],
      "metadata": {
        "id": "G3lgpIx1Ry_d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHKWbeD9upik",
        "outputId": "37f451f8-122d-4942-f85b-0dd3d0881d11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "tokenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 107kB/s]\n",
            "config.json: 100% 483/483 [00:00<00:00, 2.79MB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 479kB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 1.87MB/s]\n",
            "model.safetensors: 100% 268M/268M [00:02<00:00, 112MB/s]\n",
            "Epoch 1/1: 100% 5000/5000 [29:19<00:00,  2.84batch/s, loss=0.062]\n",
            "Evaluating: 100% 10000/10000 [01:48<00:00, 92.04batch/s]\n",
            "Epoch: 1, Loss: 0.2301, Precision: 0.9355, Recall: 0.9355, F1: 0.9355\n"
          ]
        }
      ],
      "source": [
        "!python main.py --model_type BERT --preprocess 0 --part 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgJXcQL_upik",
        "outputId": "c23a9b48-0dcc-4d83-a8f3-00200a5ac26a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Preprocessing train data: 100% 40000/40000 [00:36<00:00, 1083.66it/s]\n",
            "Preprocessing test data: 100% 10000/10000 [00:09<00:00, 1076.05it/s]\n",
            "Epoch 1/1: 100% 5000/5000 [24:29<00:00,  3.40batch/s, loss=0.258]\n",
            "Evaluating: 100% 10000/10000 [01:25<00:00, 117.26batch/s]\n",
            "Epoch: 1, Loss: 0.2659, Precision: 0.9220, Recall: 0.9216, F1: 0.9216\n"
          ]
        }
      ],
      "source": [
        "!python main.py --model_type BERT --preprocess 1 --part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT - Version 2"
      ],
      "metadata": {
        "id": "o5Ry0qZ2R164"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --model_type BERT --preprocess 0 --part 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buq7qRfIBhZj",
        "outputId": "81a761c4-ada4-4437-a72a-5cd4f13c24af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Epoch 1/1: 100% 5000/5000 [31:53<00:00,  2.61batch/s, loss=0.107]\n",
            "Evaluating: 100% 10000/10000 [01:57<00:00, 85.12batch/s]\n",
            "Epoch: 1, Loss: 0.2367, Precision: 0.9338, Recall: 0.9337, F1: 0.9337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --model_type BERT --preprocess 1 --part 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwZ7xISMBIyj",
        "outputId": "87c59359-e856-4a2a-c666-870bdd973790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Preprocessing train data: 100% 40000/40000 [00:42<00:00, 938.82it/s] \n",
            "Preprocessing test data: 100% 10000/10000 [00:11<00:00, 904.26it/s]\n",
            "tokenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 86.7kB/s]\n",
            "config.json: 100% 483/483 [00:00<00:00, 2.12MB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 3.84MB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 11.1MB/s]\n",
            "model.safetensors: 100% 268M/268M [00:02<00:00, 114MB/s]\n",
            "Epoch 1/1: 100% 5000/5000 [26:21<00:00,  3.16batch/s, loss=0.443]\n",
            "Evaluating: 100% 10000/10000 [01:35<00:00, 104.49batch/s]\n",
            "Epoch: 1, Loss: 0.2735, Precision: 0.9253, Recall: 0.9252, F1: 0.9252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT - Version 3"
      ],
      "metadata": {
        "id": "yxXVIz8ER34m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --model_type BERT --preprocess 0 --part 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9Yl-zdVR5PP",
        "outputId": "396fbe73-90ef-44e6-bd06-001e0f914d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Epoch 1/1: 100% 5000/5000 [31:41<00:00,  2.63batch/s, loss=0.0393]\n",
            "Evaluating: 100% 10000/10000 [01:56<00:00, 86.03batch/s]\n",
            "Epoch: 1, Loss: 0.2365, Precision: 0.9344, Recall: 0.9344, F1: 0.9344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --model_type BERT --preprocess 1 --part 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xkTa5_sTN49",
        "outputId": "b2b9fdfc-6c60-4409-f2b2-7bd3fb9650f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Preprocessing train data: 100% 40000/40000 [00:43<00:00, 927.67it/s]\n",
            "Preprocessing test data: 100% 10000/10000 [00:09<00:00, 1070.49it/s]\n",
            "Epoch 1/1: 100% 5000/5000 [26:25<00:00,  3.15batch/s, loss=0.436]\n",
            "Evaluating: 100% 10000/10000 [01:34<00:00, 105.97batch/s]\n",
            "Epoch: 1, Loss: 0.2739, Precision: 0.9148, Recall: 0.9120, F1: 0.9118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT - Version 4"
      ],
      "metadata": {
        "id": "p7_yG4QJqwg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --model_type BERT --preprocess 0 --part 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lc3pjR2bqy0u",
        "outputId": "d073c302-e116-45ee-dca3-c17b76247fb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "tokenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 120kB/s]\n",
            "config.json: 100% 483/483 [00:00<00:00, 1.94MB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 45.3MB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 14.5MB/s]\n",
            "model.safetensors: 100% 268M/268M [00:01<00:00, 241MB/s]\n",
            "Epoch 1/1: 100% 5000/5000 [31:31<00:00,  2.64batch/s, loss=0.0714]\n",
            "Evaluating: 100% 10000/10000 [01:57<00:00, 84.83batch/s]\n",
            "Epoch: 1, Loss: 0.2367, Precision: 0.9325, Recall: 0.9325, F1: 0.9325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT - Version 5"
      ],
      "metadata": {
        "id": "zopXBYuID3J_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --model_type BERT --preprocess 0 --part 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48nDxF7GD5E2",
        "outputId": "c738a5ed-a347-4446-a982-a0e901aa47ff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "tokenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 126kB/s]\n",
            "config.json: 100% 483/483 [00:00<00:00, 2.25MB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 3.49MB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 6.98MB/s]\n",
            "model.safetensors: 100% 268M/268M [00:01<00:00, 239MB/s]\n",
            "Epoch 1/1: 100% 5000/5000 [29:59<00:00,  2.78batch/s, loss=0.0427]\n",
            "Evaluating: 100% 10000/10000 [01:49<00:00, 91.38batch/s]\n",
            "Epoch: 1, Loss: 0.2833, Precision: 0.9231, Recall: 0.9227, F1: 0.9227\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}